/**
 * Verteilte Systeme Mitschrift
 * author: d4ryus - https://github.com/d4ruys/
 * vim:tw=80:ai:ts=4:sw=4:spell:foldmethod=expr:nonumber:norelativenumber:foldexpr=NumberFold():
 */

0. Einführung

    Verteiltes System (SW-Orientiert):
    System voneinander unabhängige Prozesse
    (SW-Komponenten), die auf verschiedenen vernetzten Rechnern zusammenarbeiten
    und einem Benutzer wie ein einziges System erscheinen.

    Verteiltes System (HW-Orientiert):
    Menge voneinander unabhängiger Kooperierenden Rechner, die einem Benutzer
    wie ein einziges System erscheinen.

    Transparenz!

    Client-Server-Modell

    Entferntes Objekt (Remote Object):
        - läuft auf Server-Machine
        - wird vom Object auf Client-Maschine benutzt über eine Schnittstelle

    Entfernte Methode: Methode eines entfernten Objektes
    Entfernte Prozedur: Prozedur/Funktionen die auf einer Server Maschine
                        aufgerufen wird.

1. Remote Method Invocation (RMI)
1.1. Einführung

    C/S-Modell

    Server-Aufgaben: erzeugt entfernte Objekte
                     macht ihre Existenz bekannt
                     macht den Zugang zu ihnen möglich
                     wartet auf Clients, die sie benutzen

    Client-Aufgaben: Verschafft sich Zugang zu entfernten Objekten, und benutzt
                     sie

1.2. RMI - Architektur

    Kommunikationsachitektur: Protokollstack
    Ablaufmodell: 1. Registrierung: bind(), ...
                  2. Lokalisierung: lookup(), ...
                  3. Aufruf

    RMI-Registry: ein auf einer Server-Maschine laufendes Programm zur
                  Verwaltung entfernter Objekte. Realisiert einen Namensdienst:
                  Name -> Objektreferenz (Abbildung).

    Name: rmi://<server-maschine>:port/<remote object name>
           |     ||                ||
      ,---´      \/                \/
      |      optional,          optional,
      | default: localhost     default 1099
      |
      `-> nur, wenn server-maschine oder port angegeben.

    Objektreferenz:  maschinenlesbarer Wort, der den Zugriff auf ein Objekt
                     möglich macht.
                     Inhalt (im Allgemeinen): Server_Adresse
                                              Portnummer
                                              Object-ID
              ,->--------------------->-.
              |                         |
    Serialisierung/Deserialisierung: Umwandlung eines Objektes in einen
                     |               Byte-Strom.
                     `->umgesetzt

1.3. RMI - Entwicklung

    4 Komponenten: - Schnittstellendefinition
                   - Schnittstellenimplementierung
                   - Server ( main() )
                   - Client ( main() )

    JDK stellt zur Verfügung:
        Interfaces: Remote, Serializable
        Classes: Remote Object, Unicast Remote Object
        Exceptions: Remote

2. Remote Procedure Calls (RPC)
2.1. Einführung

    RFC 5521,4506,1833
        RPCv2 XDR Postmapper
    C/S-Modell
    Remote Procedure Call: Aufruf einer Prozedur in einem anderen Adressraum als
                           dem des Aufrufers.
    Remote Procedure
    Remote Programms: Logisch zusammenhängende Remote Procedures

2.2. RPC - Program-Modell

    C: main() mit Prozeduren, die entfernt sein können
    Normale C-Philosophie von Prozedur-Aufrufen, und über das Netzwerk hinweg:
    "single thread of control across a network"
    Request-Reply-Mechanismus.
    Unterschied entfernter <-> lokaler Aufruf:
        - keine Zeiger-Übergabe
        - keine gemeinsame globale Variablen
        - Performance
        - nur ein Parameter bei Aufruf von entfernten Prozeduren
               `--> kann struct sein

    Identifizierung von Remote Prozeduren
        - Internet-Adresse des Server Rechners
          (pros, vers, proc)
            |     |     `--> Prozedurnummer
            |     `-->Versionsnummer von Prozedur
            `--> Programm Nummer

2.3. RPC - Ablaufmodell

    1. Registrierung von entfernten Prozeduren durch den Server
    2. Lokalisierung von entfernten Prozeduren durch den Client
    3. Aufruf

2.4. Kommunikationsmodell

    Stack gemäß Schichtenmodell
    Middleware: Stubs

2.5. externe Datenmanipulation mit Codierung

    Informationen + Parameter von Remote Prozedures müssen zur Übertragung
    codiert/decodiert werden (= ASM1 + BER)

    IDD - Datentype werden mit Hilfe von RPC-Routinen codiert/decodiert

2.6. RPC - Messages

    Call (Request) | spezifiziert in RPC-Language (C-ähnlich)
    Reply          | codiert mit Hilfe von XDR.

2.7. RPC - Entwicklungsprozess

    Zu entwickeln durch Anwender: Client, Server Prozeduren

    Im System generiert: Communication Stacks durch -> rpcgen <-
                         auf der Basis einer Schnittstellendefinition: *.x

3. Synchronisation im VS
3.1. Grundbegriffe

    Problem: - gemeinsame physikalische Zeit für Komponenten eines VS,
             - oder: allgemein im VS akzeptierte Ordnung von Aktionen und
               Ereignissen

    Lösung: Synchronisation

    Sync: Beeinflussung parallel arbeitender Prozesse dahingehen, dass ihre
          Aktionen/Ereignisse hinsichtlich ihres zeitlichen Ablaufs in eine von
          allen Prozessen akzeptierte Reihenfolge gebraucht werden.

    Zeitdienst: leitet die Sync von Prozessen in einem VS.
        `--> geleistet von einem Zeitserver.

3.2. Synchronisation physikalischer Rechneruhren
3.2.1. Systemmodell für physikalische Uhren

    Vorausgesetzt: C utc (t): Universelle Zeit, ausgegeben von einem zentralen
                              Zeitserver, z.B. NIST, PTB
                   C i (t)  : lokale Zeit eines Rechners i.
    Charakterisierung einer Rechneruhr: μ : universelle Abweichungsrate einer
                                           Rechneruhr
    Rechnerstellen gibt an:  mit: 1 - <= dc/dt <= 1t

    Bedeutung: nach 1 Zeitintervall (z.B. 1 sec) weicht die Rechneruhr um
               maximal Zeiteinheiten von der physikalischen Uhr ab.
    Bedeutung: nach 1 Zeitintervall (z.B. 1 sec) weichen 2 Rechneruhren maximal
               2 μ Zeiteinheiten voneinander ab.

    Sollen zwei Uhren mit μ nie mehr als A Sekunden auseinander liegen dann
    müssen sie spätestens mit < Δt Sek. synchronisiert werden.

    Es gilt: A = 2 μ Δt
        => Δt = A/2 μ
            `--> nach dieser Zeit muss in einem VS auf 2 Komponenten
                 synchronisiert werden!
                 Δ μ

3.2.2. Algorithmen zum Sync physikalischen Rechneruhren

    Possium Zeitserver zentraler Algorithmus:

    Possium: C utc, I , To, T, ...
    Neu Setzten der Uhr des Zeitclients: neue zeit = C utc + (Ti - To - I)/2
                                                                 (Übertrag)
    Probleme: - Zentraler Server: Überlastung, Ausfall
              - Endl. Verarbeitungszeit ( >0 ) bei Zeitclient und Zeitserver
                Abhängigkeiten der Übertragunszeiten von Zeitanfrage und antwort
                von der Netzbelastung
              - n. Cl. Zeitübersetzung der Zeitclient-Uhr:
                => Zeitgesuchte nicht mehr eindeutig.

    Aktiver Zeitserver, zentraler Algorithmus

    - beruht auf Durchschnittsbildung von Zeiten.
    - Zeitserver sendet periodisch: Zeit requests
    - Zeitclient sendet daraufhin:  Zeit replies
    - Zeitserver sendet daraufhin:  Zeitkorrekturen (Δ abhängig)

    Wesentliches Problem: "Driften" der Zeit im gesamten VS.

3.2.3. Protokolle zur Sync. physikalischer Rechneruhren

    Network Time Protokoll NTP (RFC 1205)

    NTP: Anforderungen
         Architektur besteht aus Schichten (Strata).
         Operationsmodi
         Nachrichten

    Vereinfachung: SNTP (Simple NTP) (RFC 4320)
        Nachrichten: enthalten Zeitstempel:

            0                        31
            +-------------------------+
            | Sekunden, seit 1.1.1900 |
            |-------------------------|
            |   Sekundenbruchteile    |
            +-------------------------+

        Nachrichtenaustausch:

           |  ---- Zeitrequest  ----> |
           | <---- Zeitresponse ----  |

        mit den Zeitstempeln T1, T2, T3.

        Berechnet werden: - Offset: Gangunterschied der Zeitclientuhr zu
                                  Zeitserveruhr
                          - Round Trip Delay : Zur Ermittlung der Qualität der
                                               Zeitinfo vom Zeitserver

3.3. Ereignis-Anordnung durch logische Uhren
3.3.1. Lamport-Zeitstempel

    Def. Logische Uhr: immer weiter vorwärts laufender Zeitzähler
                                                         (1, 2, 3, 4, 5, 6, ...)

    Def. Logische Uhrzeit: Wert einer Logischen Uhr beim Auftretten eines
                           Ereignisses.
    Beschreibung: Relation "geschieht vor ", " --> "
                  a, b Ereignisse
                  Dann gilt "a geschieht vor b", alle Prozesse eines VS sind
                  sich einig, dass zuerst a stattfindet und danach b.

    Def. Relation"geschieht vor", " --> ":
        Für zwei Ereignisse a, b gilt a --> b in den folgenden beiden
        Situationen:
            - a und b sind Ereignisse im selben Prozess, und a tritt vor b auf.
            - a ist ein Sendeereignis bei einem Prozess, b ist das zugehörige
              Empfansereignis bei einem anderen Prozess
        Eigenschaften von " --> ":
            - Transitivität: a --> b, b --> c => a --> c
            - es ist möglich --> (a --> b) u. --> (b --> a): nebenläufige
              Ereignisse.

    Def. Zeitstempel eines Ereignisses a:
        Zeitstempel ((a): Wert einer logischen Uhr beim Einsetzten von a.

        Anforderung: a --> b <=> C(a) < C(b)

        Sog. Lampert-Zeitstempel erfüllen: a --> b => C(a) < C(b)

        Es gilt nicht: C(a) < C(b) => a --> b

    Um (*) im laufendem Verteilen System sicherzustellen:
        Lampert - Algorithmus

    "Wenn eine Nachricht gesendet wird, muss sie logisch später ankommen, also
    z.B. um eine Zeiteinheit später. Jede Nachricht enthält also ihren
    logischen Sendezeitpunkt mitgegeben, und der Empfänger stellt bei ihrem
    Empfang seine logische Zeit, wenn nötig, auf diese Zeit + 1 vor."

3.3.2. Vektor-Zeitstempel

    Def. Eine Vektor-Uhr Vi eines Prozesses Pi in einem VS mit N Prozessen P1,
         ..., PN ist ein Array von N logischen Uhren. Der momentare Wert einer
         Vektor-Uhr Vi wird als Vi[i:N]  bezeichnet mit den Komponenten Vi[j],
         j=1, ..., N.

    Beispiel: (2, 0, 1) ist momentaner Wert einer Vektor-Uhr in einem VS mit 3
              Komponenten.

    Def. Ein Vektor-Zeitstempel Ti[I:N] eines Prozesses Pi in einem VS mit N
         Prozessen is an einem Ereignis zugewiesen oder in einer Nachricht
         übertragener momentanen Wert seiner Vektor-Uhr Komponenten eines
         Vektor-Zeitstempels: Ti[j], j=1, ..., N

    bei e Ereignis: Zugehörige Vektorzeitstempel: V(e)

    Zum Anordnen von Ereignissen in einem VS mit Hilfe von Vektor-Zeitstempeln
    existieren sog. Aktualisierungsregeln.

    Austauschen von Vektorzeitstempeln:

        V =  V' <=> V[i] =  V'[i] ∀i
        V <= V' <=> V[i] <= V'[i]
        V <  V' <=> V    <= V'    nicht (V = V')

    Es gilt: a --> b <=> V(a) < V(b)

    Vektor-Zeitstempel können nebenläufig Ereignisse charakterisieren:
                                                `--> a, b

    ¬(V(a) <= V(b)) ¬(V(b) <= V(a))  a || b

    Beispiel: (5, 0, 1), (4, 1, 7)

4. CORBA
4.1. Einführung

    C/S-Systeme: - RMI
                 - RPC
                 - CORBA
                 - ESB

    CORBA-Spezifikation: OMG spezifizierte die OMA (Object Mgt. Arch.)

    OMA ---> Core Object Modell
         `-> Reference Modell ORB (Object Request Broker) und

    Architektur des ORBs: CORBA: Common ORB Architecture

    CORBA ---> IDL Schnittstelle
           `-> Komponenten

4.2. Interface Description Language IDL

    Definition: IDL ist eine Spezifikation für Schnittstellen.

    IDL-Eigenschaften:

        - Unabhängigkeit von: Programmiersprachen,
                              Betriebssystemen,
                              HW-Architekturen

        - Reine Spez. Sprache: keine Statements
                               keine Variablen

    IDL-Grundkonstrukte: Types, ..., Operations, ..., Interfaces, ...

        Typen: Menge von Werten.

            - Objektreferenzen
            - Basistypen
            - benutzerdefinierte Typen - können z.T. zum Aufbau komplexer
                                         Typen verwendet werden

            Neue Typen definieren: typedef (typedef alter_typ neuer_typ)

        Beispiel: typedef long Kennziffer;

        Sequenzen: Arrays variabler Länge, maximale länge ist definierbar.

        Konstanten: Werte, die sich nicht ändern können.

        Exceptions: Typen, deren Werte dem Aufrufer einer Operation das
                    Auftreten von Fehlern mitteilen.

        Operations: Spezifikation von zu implementierenden Methoden bestehend
                    aus:

                    - Rückgabetype, auch void möglich
                    - Name
                    - Argumentteile (in Liste von Argumenten mit jeweils Mode,
                      Typ)
                    - Exceptions (optional)
                    Spezialfall angegebene Operationen: Operationen

        Attributes: Eigenschaften von Schnittstellen
                    Nicht unbedingt Notwendig.
                    Keine Variablen

        Interface: Typbeschreibungen
                   Vererbung zwischen Interfaces möglich
                     `--> sogar Mehrfachvererbung
                   Interfaces können nicht geschachtelt werden.

        Modules: dienen der Definition von Narmensbereichen, z.B. für mehrere
                 Interfaces
                 ! Jedem IDL-Konstante entsprechende Programmsprachen Konstrukte
                    zugeordnet, von daher: IDL Language Mapping.

4.3. Inter-ORB-Kommunikation
4.3.1. Interoperable Object References

    Notwendig für C/S-Kommunikation unter CORBA: IORs

    Bestandteile: Schnittstelleninformation
                  Profildaten (häufig über Server-Objekte: Adressen...Object
                     `-->vielfach möglich                              Key.)

4.3.2. GIOP und IIOP
       |        `--> Internet über ORB Protocol (Spezialisierung von GOP auf
       |                                         Internet Kommunikation))
       `--> gesendet über ORB-Protocol

    IIOP spezifiziert: Nachrichtenformate
                       Codierungsregeln: CDR Common Data Representation

    Ablauf der CORBA-Kommunikation mit Hilfe eines IOR:
        Ein Client besitzt die IOR eines Server-Objekts, das es ansprechen
        möchte. Dann baut er mit Hilfe von Internet-Adresse und
        Post-Nummer in der IOR eine TCP-Verbindung auf. Danach auf dieser
        TCP-Verbindung Austausch von Request und Reply. Dabei enthalten die
        Requests: Object Key des Server Objekts, die Operation und ihre
                  Argumente.

4.4. CORBA Namensdienst

    Namensdienst realisiert eine Abbildung: Name -> Menge von Attributen
                                       z.B. Name -> {Objektreferenz}
                                            Name -> {Adresse}

    Begriffe:

        Namenskomponente: 2 Strings, der 2 wird nicht benutzt.

        Namensbindung:    Paar: Namenskomponente, Objektreferenz

        Namenskontext:    Menge von Namensbindungen

        Objektrefnummern können referenzieren: Anwendungsobjekte, weitere
                                               Namensobjekte
            => Namensgraphen

        Name: Folge von Namenskomponenten: Kontextname, .... Kontextname
                                           (elementarer Name)

        Namensdienst unterstützt:
            - resolve() von Client: finden einer Objektreferenz
            - send(), rebind() vom Server: Erschaffung eines Namensbindung

        Zusätzlich um eine Referenz auf den Namensdienst zu erlangen:
            resolve_initial_references()

5. Verteilte Datenbanken und verteilte Transaktionen
5.1. Grundbegriffe

    Def. Eine Verteilte DB ist eine DB deren Daten physisch auf mehrere
         Standorte (Rechner) verteilt sind, währen logisch gesehen dem Benutzer
         eine einzige DB präsentiert wird.

    An den einzelnen Standorten: lokale Datenbanken bzw. "Datenbankteile"

    Def. Eine verteilte Transaktion ist eine Transaktion, die auf den
         Datenbestand einer Verteilten DB arbeitet.

        Nachricht von verteilten DB: Komplexität!

    Def. Replication ist die mehrfache Speicherung von Daten an verschiedenen
         Standorten einer verteilten DB.
         Diese Daten heißen: Replikate, Duplikate, Kopie, Primary Copies, Slave
                             Copies

5.2. Entwurf von verteilten DBs

    mehrere Schritte:
        - normales: DB-Schema
        - Fragmentation: die Aufteilung von Daten einer DB in logische Einheiten
                         (Fragmente), beschreiben durch ein Fragmentationsschema
        - Allokation: Zuweisung von Fragmenten der physisch verschiedene
                      Standorte ein verteilten DB -> Allokationsschema
        - Replication: Erstellung und Verwaltung von Replikaten
                            -> Replikationsschema.

    zu Fragmentation bei relationalen Datenbanken:

        horizontale Fragmente: Zeilen, definiert durch (sigma)
            Beispiel: (sigma) (MITARBEITER)

        vertikale Fragmente: Spalten, definiert durch (pi)
            Beispiel: (pi vname nname) (MITARBEITER)

        gemischte Fragmente: definiert durch (pi), (sigma):
            Beispiel: (sigma)(pi) (MITARBEITER)

        +------------------------------------------------------+
        | Verteilung: Fragmentation + Allokation + Replikation |
        +------------------------------------------------------+

    Die Fragmentation ist eine Menge von Metadaten in Form von
        Guard Conditions + Attribute-Listen
          (sigma)                    (pi)
             |                        |
             +---> rel. Algebra >-----+

5.3. Anfragen in verteilte Datenbanken

    Problem: Minimierung von Datentransferkosten von Anfragen
    Problem: Dekomprition von Anfragen

5.4. Verteilte Transaktionen
5.4.1. Begriffe und Transaktionsmodell

    Motivation: Transaktionsabwicklung wiederum notwendig: ACID!

    Modell einer verteilten Transaktion:

        - lokale Transaktionen, die auf lokalen Datenbank-Teilen arbeiten:
          Participants, Worker kontrolliert durch einen lokalen
          Transaktionsmanager

        - ein Steuerprozess der das nebenläufige Arbeiten und eine eventuelle
          Recovery aller lokalen Transaktionen steuert:
            Koordinator

        Zusätzlich: 1 oder mehrere lock-Manager
        Grundprinzip:
            Eine verteilte Transaktion endet, wenn entweder:
                - alle ihre lokalen Transaktionen positiv terminieren
                - (comitten) oder negativ terminieren (aborten).

5.4.2. Concurrency Control durch 2PL-Verfahren
                                  `-> Two Phase Locking
    Praktisch verwendet: mehrere 2PL-Verfahren

    Alle involvieren: ein oder mehrere Lock Manager
                      ein Transaktions-Koordinator
                      lokale Transaktions-Manager, die jeweils lokal das
                          2PL-Protokoll umsetzten.

    Zentralisiertes 2PC:

        1 Transaktions-Koordinator
        1 zentraler Lock-Manager
        n lokale Transaktions-Manager

        2 Varianten: mit oder ohne Replikate von DB-Elementen.

    Primary Copy 2 PL

        1 Transaktions-Koordinator
        m Lock-Manager (m < n)
        n lokale Transaktions-Manager

    für jedes DB-Element:

        1 Primary Copy
        mehrere Slave Copies.

    Verteiltes 2PL:

        1 Transaktions-Koordinator
        n lokale Lock-Manager
        n lokale Transaktions-Manager

5.4.3. Recovery Control durch 2PC-Verfahren

    2PC stellt sicher: Global Commit Rule:
        - eine verteilte Transaktion kann nur dann commiten,
          wenn alle ihre lokalen Transaktionen committed haben
        - eine verteilte Transaktion muss aborten, wenn nur eine ihrer lokalen
          Transaktionen aborted.

    2PC hat 2 Phasen:
        Voting   Phase: Vorbereitung des Beendens einer verteilten Transaktion,
        Decision Phase: Beenden einer verteilten Transaktion

    Anlaufen des 2PC:
        - Koordinator hat den Participants genug Zeit gegeben,
        - Participants haben die Willen zur Beendigung signalisiert,
        - der Anwender der verteilten Transaktion hat "Ende" signalisiert.

    Beispiel für 2PC-Implementierung: CORBA-Transaktionsdienst.

    In der Praxis benutzt: primary copy 2PL + 2PC
                           für ACID mit einem verteilten System!

6. Grundalgorithmen in verteilten Systemen
6.1. Wahlalgorithmen

    Problem: Koordination ausgefallen,
             neuer Koordinator muss bestimmt/gewählt werden.

             => Algorithmus für Neuwahl nötig.

    Voraussetzung:
        - jeder Prozess im VS hat eine eindeutige Nr. oder ID,
          nach der die Prozesse geordnet werden können,
        - jeder Prozess kennt die Num. bzw. IDs aller anderen Prozesse des VS
        - kein Prozess im VS weiß über die Zustände der anderen Prozesse
          Bescheid ("arbeitet gerade", "wartet", "ausgefallen", ...)

    Algorithmen: Bully-Algorithmus
                 Ring-Algorithmus: Participants sind in einem logischem Ring
                                   organisiert.

6.2. Wechselseitiger Ausschluss

    Grundbegriffe: Betriebsmittel
                   Kritischer Bereich (kritischer Abschnitt)
                     `--> Benutzung: Eintritt
                                     Arbeiten u.a. Zugriffe auf Betriebsmittel

    Anforderungen an Algorithmen für den wechselseitigen Ausschluss:
        - Exklusivität
        - Liveness
        - Reihenfolge-Einhaltung

6.2.1. Zentraler Algorithmus

    - Koordinator mit Warteschlange
    - mehrere Konkurierende Prozesse
    - Messages: Request- Grant- Release-Ressource

    Nachteile:
        - Koordinator ist zentraler Ausfallpunkt
        - Anforderung der Reihenfolge-Einhaltung ist nicht garantiert.

6.2.2. Verteiler Algorithmus

    - Kein Zentraler Koordinator
    - nur gleichberechtigte Anwendungsprozesse
    - Messages: Request, OK
    - Ablauf: Wenn ein Prozess eine Ressource anfordern möchte, schickt er ein
        Request mit Prozessnummer, gewünschter Ressource mit Zeitstempel an alle
        Prozesse des verteilten Systems, also auch an sich selbst. Benutzung
        einer Ressource bedeutet, das ein Prozess sich im entsprechenden
        kritischen Bereich befindet.
        Reaktion auf ein Request:
            - Befindet sich der Empfänger im angeforderten kritischen Bereich,
              antwortet er nicht und stellt die Anforderung in seine
              Warteschlange.
            - Befindet sich der Empfänger nicht im kritischen Bereich und will
              auch nicht in ihn eintreten, sendet er eine OK-Nachricht zurück,
            - Befindet er sich nicht im kritischen Bereich, will aber in ihn
              eintreten so vergleicht er den Zeitstempel aus dem eingehenden
              Request mit dem Zeitstempel aus der Request-Nachricht, die er
              selbst an die anderen Komponenten des Systems und an sich selbst
              geschickt hat. Je nach Zeitstempel sendet er eine OK-Nachricht
              zurück (eigene Anforderung nicht zurückgestellt), oder er sendet
              nicht und stellt eigene Anforderung in die eigene Warteschlange
              (eigene Anforderung hat Vorrang).
    - Nachteile:
        - hohes Nachrichtenaufkommen (Requests, OKs)
        - alle Prozesse sind kritische Ausfallpunkte

6.2.3. Ring-Algorithmus

    - Token: Exklusivrecht auf kritischen Bereich
    - Token kreist in logischem Ring.
    - Nachteile:
        - Reaktion bei Token-Verlust
        - Repräsentiert den Besitz eines Tokens
        - Exklusivrecht auf: - eine
                             - mehrere kritischen Bereiche im System?
                             - alle

6.3. Deadlocks

    problematische Systemzustände
    4 notwendige Bedingungen für Deadlocks
    Deadlock-Behandlung:
        - Deadlock-Erkennung und -Auflösung
        - Deadlock-Vermeidung
        - Deadlock-Verhinderung

6.3.1. Deadlock-Erkennung mit Auflösung

    Belegung-Anforderung-Graph:
        - zwei Arten von Knoten: Prozesse, Ressourcen
        - zwei Arten von Kanten: P -> R => Prozess hat Ressource angefordert
                                 R -> P => Ressource ist von Prozess belegt
                                           worden

    Deadlock-Erkennung durch zentrales Verfahren:
        - beteiligte Systeme (Komponenten) senden ihren lokalen
          Belegungs-Anforderungs-Graphen an einen Zentralen Koordinator.
          Koordinator kombiniert die Graphen und sucht nach Zyklen. Bei
          Vorhandensein eines Zyklus: Abbruch mindestens eines Prozesses.

    Deadlock-Erkennung durch verteiltes Verfahren: Edge Chasing.
     `--> verwendet: Management-Nachricht: Probe

    Ein involvierter Prozess erkennt über den Empfang einer Probe, dass ein
    Zyklus vorliegt, und beendet sich.

7. Replikation und Konsistenz
7.1. Grundlagen

    Replikation: Halten und Verwalten von mehreren Exemplaren von Daten.
                                                    `--> Replikate

    Konsistenz: Daten in einem VS sind konsistent wenn sie innerhalb des VS
                widerspruchsfrei sind. D.h.: sie stoßen nicht gegen ein
                definiertes Konsistenzmodell

    Replikationsvorteile: Referenz-Verbesserung
                          Verfügbarkeit
                          Fehlertoleranz

    Replikationsnachteil: Komplexität

7.2. Konsistenzmodelle

    System-zentrierte (Daten-zentrierte) Konsistenzmodelle:
        stellen eine systemweit konsistente Sicht auf die Daten in einem VS
        bereit.

    Benutzer-zentrierte (Client-zentrierte) Konsistenzmodelle:
        bieten Konsistenz-formatieren (nur) aus Sicht eines einzelnen
        Benutzers.

7.2.1. System Kons-Modelle

    Strikte (strenge) Konsistenz:
        Jede Leseoperation auf ein Datenelement x liefert das Ergebnis der
        letzten Schreiboperation auf x zurück
        ... zu streng um praktikabel zu sein.

    Sequenzielle Konsistenz:
        Wenn mehrere nebenläufige Prozesse auf Daten zugreifen, dann ist jede
        Folge von Lese- und Schreiboperationen akzeptabel, solange alle
        Prozesse dieselbe Folge setzen.

    Kausale Konsistenz:
        Die Resultate von Schreiboperationen, die potentiell kausal voneinander
        abhängen, müssen für alle Prozesse in derselben Reihenfolge sichtbar
        sein.

        Beispiel für potentiell kausal abhängige Schreiboperationen:

            W1(x)a; R2(x)a; ...; W2(x)b               <-.
               `-------´     `--> Berechnung von b      |
                kausal       auf Basis von a (möglich)  |
               abhängig                                 |
                  `-------------------------------------´
                  möglich: die kausale Abhängigkeit: potentiell kausal abhängig

    Es gilt: Kausal K  < Sequentielle K < Strenge K.

    Weitere K-Modelle: linearisierbarkeit - K.
                       FIFO - K.
                       Schwache - K.
                       Freigabe - K.
                       Eintritte - K.

7.2.2. Benutzer-zentrierte Konsistenzmodelle

    Eventuelle Konsistenz:
        Alle Replikate in einem VS werden irgendwann konsistent, wenn längere
        Zeit keine von Clients ausgehender Aktualisierung statt findet.

    Monotone Lesekonsistenz:
        Wenn ein Benutzer den Wert eines Datenelements x liest (egal welches
        Replikat)  gilt jede nachfolgende Leseoperation auf x durch diesen
        Benutzer, egal auf welcher Replikat von x immer denselben oder einen
        aktuellen Wert zurück.

    Monotone Schreibkonsistenz:
        Eine Schreiboperation durch einen Benutzer auf ein Datenelement x muss
        auf allen Replikationen von x abgeschlossen sein bevor ein nachfolgende
        Schreiboperation auf x durch denselben Benutzer, egal auf welchem
        Replikat, stattfindet.

7.3. Verteilung- und Konsistenzprotokolle

    V-Protokoll: Protokoll, das für die Verteilung von Aktualisierungen von
                 Replikaten in einem Verteilten System sorgt.

    Beispiel: Primär-Backup-Protokoll
    Phasen: Anforderung: Client initiiert Schreibop.
            Weiterleitung: Schreibanforderung wird an Primary Copy
                           weitergeleitet
            Anführung: Die Anforderung geht an die lokalen Kopien und wird
                       bestätigt.
            Bestätigung: Schreib-Op wird dem schreibenden Client bestätigt.


            Das Primäre Debug Protokoll implementiert die sequentielle
            Konsistenz

    Quorum-basierte Replikation:
        Von lesen bzw. Schreiben müssen Cleitns sich eine ausreichende Menge
        von Berechtigungen zum Zugriff auf Replikate beschaffen (sog. Quronen)

        Bedingungen: Schreibquoronen Nw > N²/2 <-- Anzahl der Rplikate
                                                   von x im VS
                     Lesequoronen Nw + Lesequoronen Nr > N

8. Verteilte Dateisysteme
8.1. Einführung

    Verteiltes Dateisystem: SW-System zur Unterstützung der gemeinsamen Nutzung
    von Information, die ein in einem Rechnernetz verteiltem Dateien gespeichert
    sind.

    Zweck eines verteilten Dateissytems: transparenter entfernter Datenzugriff

    stellt zur Verfügung: Dateien (File Service)

    Beispiele: NFS: Network File System
               DFS: Distributed File System

8.2. NFS

    Prinzip: Beim NFS stellt ein File Server Teile seines Dateisystems anderen
             Rechnern zum transparenten Zugriff zur Verfügung. D.h.: Ein File
             Server exportiert an andere Rechner ein Verzeichnis zusammen mit
             seinen Unterverzeichnissen und Dateien. Ein Client-Rechner kann
             dieses Verzeichnis logisch in sein Dateisystem einhängen.

8.2.1. NFS-Architektur-Modell:

    Verteiltes File-System zur Differenzierung der Zugriffe auf lokale bzw.
    entfernte Dateien.

    NFS-Client / NFS-Server
    Kommunikation über: RPC (Remote Procedure Call)

8.2.2. NFS-Dateisystem-Modell

    - Dateien besitzen unter NFS Attribute
    - NFS enthält Dateisystem Operationen, sehr unterschiedlich für die
      gängigen NFS-Versionen 3 und 4.
    - Datei-Handle: Referenz auf eine Datei in einem (möglicherweise
      entfernten Dateisystem.
        - identifiziert eine über NFS zugreifbare Datei (kann auch Verzeichnis
          sein) eindeutig.
        - wird von (File -) Server erzeugt.
        - wird von Client benutzt, ist unter für ihn nicht lesbar (opaque).

8.2.3. NFS Mount Protokoll

    Zweck: Einhängen eines entferntes Teilbaums in ein lokales Verzeichnis:
           Mount Point.

    Mittel: Mount Kommando.
            Format: mount -t nfs [...] server:/directorypath mountpoint

    NFS-Version 3 ist nicht sehr effizient: lookup() remote `
                                            ...              | viele entfernte
                                            lookup() remote  | Operationen
                                            read()   remote ´

    Höhere Performance bei NFS Version 4: zusammengesetzte Prozeduren.

8.3. DFS

    NFS: ermöglicht Zugriff auf entfernte Dateien ohne ihren physikalischen
         Standort zu ändern.

    DFS: Client kopiert gewünschte Dateien auf seinen Rechner.

    DFS-Begriffe: - DFS-Client: Maschine die auf Dateien in einem File Set des
                                File Servers zugreift
                  - DFS-Server: Maschine die Dateien eines File Set's zur
                                Verfügung stellt.

    DFS-Fileorganisation: Dateien in Verzeichnissen in File Set's und
                          Aggregation

    File-Zugriff über: File Access Model      ,--> "exklusiver Besitz"
                       File Sharing Model (verwendet Tokens)

9. Verteilte Hashtabellen (Distributed Hash Tables, DHTs)
9.1. Einführung Hashing

    Hashfunktion: Abb.: große Schlüsselmenge -> kleine Zielmenge (Menge von
                                                Hashwerten, Hashräumen)
                  h: K -> Z
                  h(k) = k und p (p i.H. Primzahl)

    Anwendungsgebiete: Speicherzuweisung an zu speichernde Objekte
                       Prüfsummenbezeichnung.

    Eine Hashfunktion h:K -> Z liefert eine Hashtabelle der Größe |Z|.

9.2. Konsistentes Hashing

    Voraussetzung: viele Daten, viele DB-Server
    -> Probleme: Welche Server sollen welche Daten speichern?
                 Wie werden Daten bei neu hinzukommenden bzw. ausfallenden
                 Servern neu verteilt?

    -> Verbesserung: konsistentes Hashing

    Prinzip: Sowohl die Server (identifiziert durch einen Schlüssel) als auch
             die Daten (ebenfalls identifiziert durch einen Schlüssel) werden
             über eine eigene Hashfunktion auf einen vorgegebenen Hashraum
             abgebildet. Dann werden die Server für ganz bestimmte Bereiche
             ("um ihrerm Hashwert herum") verantwortlich gemacht und ihre Daten
             sind die jeweils in diesem Bereich gespeichert.

9.3. DHTs

    Prinzip: viele Daten werden auf mehreren Servern gespeichert.
             Daten sollen mit Hilfe von Hashing verteilt und gefunden werden
             Die hierfür verwendete Datenstruktur (~Hashtabelle) soll verteilt
             sein. => verteilte Hashtabelle (Datenstruktur)

    Andere Def. Hastabelle: Das verteilte System aus vielen Datenbankservern
                            mit dieser Datenstruktur.

    Beispiele für solide Segmente: Chord, Pastry, CAN, ...

9.4. Chord

    Schlüsselraum: {0, ..., 2^n -1} m = Bitlänge des Schlüssels.

    Ein Server speichert die Daten mit allen Schlüsseln für die er
    verantwortlich ist

    Hashfunktion: h Server: {1, ..., n} -> {0, ...., 2^m -1}
                  h Daten:  {1, ..., k} -> {0, ...., 2^m -1}

    Speicherprinzip:

        +--------------+  im Uhrzeigersinn   +-------------------------+
        | Datenelement |-------------------->| verantwortlicher Server |
        +--------------+                     +-------------------------+

        Der für sein Datenelement verantwortliche Server ist derjenige, der im
        Uhrzeigersinn ausgehend von Datenelement als erster angetroffen wird.

    Chord implementiert:
        - Such-Prozedur für Daten
        - Einfüge/Herausnahme - Prozedur für Server
        - "Stabilisierungstsprozedur"

    durch sog. Fingertabellen:
        Einträge: (Index, Sprungadresse, nächster Server)
                   |      |                    |
                   i      (i+2^id) mod 2^m     successor

    Such-Prozedur für Daten:
        Wenn ein Server ein Request für ein Datenelement id erhielt:
            - prüft er zunächst, ob es bei ihm lokal gespeichert ist.
            - andernfalls prüft er, ob sein Nachfolger (1. Zeile Fingertabelle)
              verantwortlich ist.
            - andernfalls reicht er den Request an den Server mit der größten
              Sprungadresse weiter, die id nicht übersteigt, dieser arbeitet
              analog mit der Such-Prozedur.

9.5. Verwendung von DHTs

    Amazon Dynamo, BitTorrent, Twitter, ...

10. Lokalisionsdatenbanken
10.1. Einführung

    Motiv: In Mobilfunknetzen sind Lokalisationsinformationen zu speichern:

    Lokations-Datenbanken: Datenbasen, die die Aufenthaltsorte (Location Areas)
                           von mobilen Geräten speichert.

    Anforderung: Schnelle Suche nach mobile Geräte
                 Effiziente Aktualisierung

    Lok-DB sind i.A. verteilt.

    Model: Lokationsarchitektur: innere Knoten können lok-DB besitzen,
            Blätter: Zugangspunkte für mobile Geräte

    lok-DB können enthalten:
        "genau"    Info: Zugangspunkt eines mobilen Gerätes
        "ungenaue" Info: z.B. die aktuelle Location Area eines mobilen Geräts
        "Verweis"  Info: Hinweis auf weitere Lok-Infos.

10.2. Basisoperationen auf Lokationsdatenbanken

    Suchen eines mobilen Teilnehmers/Gerätes
        - im Baum von unten nach oben bis zum ersten gemeinsamen Knoten von
          rufenden mit gerufenen mobilen Gerät
        - aufwendig, wenn die mobilen Geräte weit auseinander liegen
        - schnell bei möglichst vielen "genauen" Informationen.

    Wechseln der Lokation
        - Knoten im Baum müssen auf zu ändernde Einträge durchsucht werden
        - es kann geändert werden: lokation selbst
                                   die Art des Eintrags: "genau", "ungenau",
                                   "Verweis"

10.3. Optimierung der Basisoperationen

    Replikation von lok-Infos: Redundanz
        Beispiel: Häufiger Aufruf von Zugangspunkt 15 auf mobiles Gerät x
                  (Folie 10.4)
            => günstig: Lok-Info im Knoten L: (x, 2)

    Caching von lok-Infos:
        Aufgrund eines Verbindungsaufbau-Vorgangs wird eine lok-Info zusätzlich
        in einer lok-DB gepeichert.

        Beispiel: 1. Verbindungsaufbau von Zugangspunkt 15 auf Zugangspunkt 2
                      => lok-Info für das Mobilgerät an Zugangspunkt 2 ist
                         bekannt bei 15 oder Knoten L => Speicherung von (x, 2)

    Nachsendeauftrag:

        Verweis, der beim (temporären) Lokationswechsel eines mobilen Gerätes
        vom Benutzer angelegt wird, mit der Absicht begrenzter Gültigkeitsdauer

    -> Praxis (GSM):
        2 Hierarchieebenen von lok-DB: HCR VLRs
